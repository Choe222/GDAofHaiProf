import torch 
from torch.optim import Optimizer

class convergence(Exception):
    pass

class GDA(Optimizer):
    def __init__(self, params, lr=0.01, sigma=0.5, k=0.75, projFunc=None):
        if lr <= 0.0:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not (0.0 < sigma < 1.0):
            raise ValueError(f"Invalid sigma: {sigma}")
        if not (0.0 < k <= 1.0):
            raise ValueError(f"Invalid k: {k}")
        self.projFunc = projFunc
    
        defaults = dict(lr=lr, sigma=sigma, k=k)
        super(GDA,self).__init__(params,defaults)
        self.state['global'] = dict(prev_lr=lr)
        
    def step(self, closure=None):
        if(closure == None):
            raise ValueError(f"This optimizer need the the before&current loss.")
        
        group = self.param_groups[0]
        sigma = group['sigma']
        k = group['k']

        global_state = self.state['global']
        prev_lr = global_state.get('prev_lr', group['lr']) 
        
        loss_old = closure()
        sump = 0.0          
        check = True
        with torch.no_grad():
            for g in self.param_groups:
                for p in g['params']:
                    if p.grad is None:
                        continue
                    grad = p.grad
                    x_old = p.data.clone()
                    p.add_(grad, alpha=-prev_lr)
                    if self.projFunc is not None:
                        p.copy_(self.projFunc(p.data))
                    diff = x_old - p.data  
                    if diff.abs().sum() > 0:
                       check = False

                    sump += (diff * grad).sum().item()

        if check == True:
            #return loss_old 
            raise convergence #if the target func is convecx?)
        loss_new = closure()   

        if loss_new.item() > loss_old.item() - sigma * sump:
            new_lr = prev_lr * k
            global_state['prev_lr'] = new_lr
            group['lr'] = new_lr
        else:
            global_state['prev_lr'] = prev_lr
            group['lr'] = prev_lr

        return loss_new
            
       import torch
import numpy as np
from scipy.optimize import minimize, Bounds
import time
from scipy.optimize import Bounds
import autograd.numpy as np1


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using:", device)

def getLr(n):
    beta = 0.741271
    alpha = 3*beta**(3/2)*(n+1)**(1/2)
    return 4*beta**(3/2)*n**(1/2) + 3*alpha

def f(x: torch.Tensor, a: torch.Tensor ,e: torch.Tensor):
    beta = 0.741271
    alpha = 3*beta**(3/2)*(x.shape[0]+1)**(1/2)

    xx = x @ x  # dot product
    k = a @ x + alpha * xx + (beta * (e @ x)) / torch.sqrt(1 + beta * xx)
    return k

EPS = 1e-8

def g1_log(x):
    x = np.asarray(x, dtype=np.float64)
    if np.any(x <= 0):
        return -np.inf
    return np.sum(np.log(x))  # >= 0

def g1_log_jac(x):
    x = np.asarray(x, dtype=np.float64)
    return 1.0 / x  

cons = ({
    "type": "ineq",
    "fun": lambda x: np.array([g1_log(x)]),
    "jac": lambda x: np.array([g1_log_jac(x)])
})

bounds = Bounds(EPS*np.ones(1), np.inf) 

def rosen(x, y):
    x = np.asarray(x)
    y = np.asarray(y)
    return np.sqrt(np.sum((x - y) ** 2))

def find_min(y: torch.Tensor):
    y_np = y.detach().cpu().numpy().astype(np.float64)
    n = y_np.shape[0]
    x0 = np.random.rand(n)

    local_bounds = Bounds(EPS*np.ones(n), np.inf)

    res = minimize(
        rosen,
        x0,
        args=(y_np,),
        jac="2-point",
        constraints=cons,
        bounds=local_bounds,
        method="trust-constr",
        options={"disp": False},
    )
    return torch.from_numpy(res.x).to(dtype=y.dtype, device=y.device)

# --- MAIN LOOP ĐÃ SỬA (THÊM CLIP GRAD) ---

print("\n" + "="*105)
print(f"Table 1  Computational results for Example 3")
print("="*105)
print(f"{'n':<5} | {'Algorithm GDA (proposed)':^45} | {'Algorithm GD':^45}")
print(f"{'':<5} | {'f (x*)':^15} {'#Iter':^10} {'Time':^15} | {'f (x*)':^15} {'#Iter':^10} {'Time':^15}")
print("-" * 105)

n_list = [10, 20, 50, 100, 200, 500]

for val_n in n_list:
    # Setup dữ liệu
    x_init = torch.rand(val_n, dtype=torch.float32, device=device)
    a = torch.rand(val_n, dtype=torch.float32, requires_grad=True, device=device)
    e = torch.arange(1, val_n + 1, dtype=torch.float32, device=device)

    # ==========================
    # PHẦN 1: GDA
    # ==========================
    lr = 5/getLr(val_n)
    x_gda = x_init.clone().detach().requires_grad_(True)
    
    params = [x_gda]
    optimizer = GDA(params=params, lr=lr, sigma=0.5, k=0.75, projFunc=find_min)

    best_gda = float('inf')
    iters_gda = 100
    
    t1 = time.time()
    try:
        for it in range(100):
            def closure():
                optimizer.zero_grad()
                F = f(x_gda, a, e)
                F.backward()
                # === QUAN TRỌNG: Kẹp Gradient để tránh NaN ở n=500 ===
                if(val_n==500):
                    torch.nn.utils.clip_grad_norm_([x_gd], max_norm=1000.0)
                return F
            
            F = optimizer.step(closure)
            
            # Kiểm tra F có phải NaN không trước khi so sánh
            if torch.isnan(F):
                break
                
            f_val = F.item()
            if(f_val < best_gda):
                best_gda = f_val
                
    except convergence:
        iters_gda = it + 1
    except Exception as ex:
        # Bắt các lỗi khác nếu có để không dừng chương trình
        pass
    
    time_gda = time.time() - t1

    # =========================
    # PHẦN 2: GD
    # =========================
    lr = 1/getLr(val_n)
    x_gd = x_init.clone().detach().requires_grad_(True)
    
    params = [x_gd]
    optimizer = GDA(params=params, lr=lr, sigma=0.5, k=1, projFunc=find_min)

    best_gd = float('inf')
    iters_gd = 100
    
    t1 = time.time()
    try:
        for it in range(100):
            def closure():
                optimizer.zero_grad()
                F = f(x_gd, a, e)
                F.backward()
                # === QUAN TRỌNG: Kẹp Gradient ===
                if(val_n==500 ):
                    torch.nn.utils.clip_grad_norm_([x_gd], max_norm=1000.0)
                return F
            
            F = optimizer.step(closure)

            if torch.isnan(F):
                break

            f_val = F.item()
            if(f_val < best_gd):
                best_gd = f_val
                
    except convergence:
        iters_gd = it + 1
    except Exception:
        pass
        
    time_gd = time.time() - t1

    # In kết quả
    # Nếu best_gda vẫn là inf (do lỗi NaN từ đầu), in ra NaN để biết
    str_gda_f = f"{best_gda:.4f}" if best_gda != float('inf') else "NaN"
    str_gd_f = f"{best_gd:.4f}" if best_gd != float('inf') else "NaN"

    print(f"{val_n:<5} | "
          f"{str_gda_f:^15} {iters_gda:^10} {time_gda:^15.4f} | "
          f"{str_gd_f:^15} {iters_gd:^10} {time_gd:^15.4f}")

print("="*105)
