def GDA_update(model, loss_fn, x, y, lambda_lr, sigma=0.5, kappa=0.5):
    for param in model.parameters():
        param.grad = None
    outputs_k = model(x)
    loss_k = loss_fn(outputs_k, y) 
    loss_k.backward()
    inner_product = 0
    with torch.no_grad():
        for param in model.parameters():
            t=param.data.clone()
            grad = param.grad
            param.add_(param.data,alpha=-lambda_lr)
            diff = t - param.data
            inner_product += (grad * diff).sum().item() 

    outputs = model(x)
    loss = loss_fn(outputs, y)

    descent_condition = loss <= (loss_k - sigma * inner_product)
    if descent_condition:
        new_lambda = lambda_lr 
    else:
        new_lambda = kappa * lambda_lr 

    return new_lambda, loss_k.item()
